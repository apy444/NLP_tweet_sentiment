{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy as sp\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from tqdm import tqdm\n",
    "# Remove punctuations, numbers, space, symbols, organizations\n",
    "# conda install -c conda-forge spacy\n",
    "# pip install spacy && python -m spacy download en\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data, setting encoding and column names\n",
    "def read_sentiment140(path):\n",
    "    columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "    df = pd.read_csv(path, encoding=\"ISO-8859-1\", names = columns)\n",
    "    df = df[[\"target\", \"text\"]]\n",
    "    # Replacing 4 with 1 in target/sentiment column\n",
    "    df.target = df.target.replace(4,1)\n",
    "    return df\n",
    "\n",
    "# Reading in data, setting encoding and column names\n",
    "def read_stocks(path):\n",
    "    columns = [\"ticker_symbol\", \"tweet_id\", \"writer\", \"body\", \"comment_num\", \n",
    "           \"retweet_num\", \"like_num\", \"company_name\", \"sector\", \"year\", \"month\", \"day\", \"hour\"]\n",
    "    df = pd.read_csv(path, header = 0, engine = 'python', \n",
    "            error_bad_lines=False, warn_bad_lines=False)\n",
    "    df.rename(columns={\"body\": \"text\"}, inplace= True)\n",
    "    # df = df[[\"target\", \"text\"]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove urls from tweets like http:// or https://\n",
    "def remove_urls(text, pattern):\n",
    "    matches = re.findall(pattern, text)\n",
    "    for m in matches:\n",
    "        text = text.replace(m, '')\n",
    "    return text\n",
    "\n",
    "def keep_alnum(text):\n",
    "    '''Keep alphabetical and numerical characters including single quote\n",
    "    Returns the text in lowercase'''\n",
    "    return ''.join([l.lower() for l in text if l.isalnum() or l.isspace() or l==\"'\"])\n",
    "\n",
    "# Remove '@' and '#' from the beginning of the words\n",
    "def remove_hashtags(text):\n",
    "    return ' '.join([w[1:] if w[0] in ['@', '#'] else w for w in text.split()])\n",
    "\n",
    "# Count the number of negating words like not, none, don't ...\n",
    "def count_negation(text, negating_w):\n",
    "    '''Returns 1 if there is a negation in the sentence, otherwise returns zero'''\n",
    "    count = sum([1 for w in keep_alnum(text).split() if w in negating_w])\n",
    "    return int(bool(count))\n",
    "\n",
    "def count_pos(text, pos):\n",
    "    return sum([1 for w in keep_alnum(text).split() if w in pos])\n",
    "\n",
    "def count_neg(text, neg):\n",
    "    return sum([1 for w in keep_alnum(text).split() if w in neg])\n",
    "\n",
    "def tokenize(text, pos_list, stop_words, symbols):\n",
    "    # ent_list = ['ORG']\n",
    "    punctuations = string.punctuation\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = [token.lemma_ for token in doc if token.pos_ not in pos_list \n",
    "                                           and token.lemma_!= \"-PRON-\"]\n",
    "    \n",
    "    tokens = [token.lower().strip() for token in tokens \n",
    "              if token.lower() not in stop_words \n",
    "              and token.lower() not in punctuations\n",
    "              and token.lower() not in symbols]\n",
    "    \n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df_orig):\n",
    "    \n",
    "    df = df_orig.copy()\n",
    "    negating_w = ['no', 'not', 'none', 'nobody', 'nothing', 'neither', 'nor', 'nowhere', 'never', 'hardly',\n",
    "             'scarcely', 'barely', \"doesn't\",\"doesn'\", \"don't\",\"don'\", \"isn't\",\"isn'\", \"wasn't\", \"wasn'\",\n",
    "             \"shouldn't\", \"shouldn'\", \"wouldn't\", \"wouldn'\", \"couldn't\", \"couldn'\", \"won'\", \"hadn'\",\n",
    "             \"won't\", \"can't\", \"cannot\", \"hadn't\", \"hasn't\", \"aren't\", \"didn't\", \"haven't\", \"mightn't\",\n",
    "             \"needn't\", \"shan't\", \"hasn'\", \"didn'\", \"haven'\", \"mightn'\", \"needn'\", \"shan'\", \"aren'\"]\n",
    "    pattern = re.compile('https?://[^\\s]+')\n",
    "    pos = pd.read_csv('opinion_lexicon/positive-words.txt', header=None)[0].tolist()\n",
    "    neg = pd.read_csv('opinion_lexicon/negative-words.txt', header=None)[0].tolist()\n",
    "    # https://universaldependencies.org/u/pos/\n",
    "    pos_list = ['PART','PUNCT', 'NUM', 'CARDINAL NUM', 'SPACE', 'SYM', 'X']\n",
    "    stop_words = (set(stopwords.words('english')) \n",
    "                  - set(['more', 'up', 'down', 'most', 'over', 'above', 'under', 'no', 'not']))\n",
    "    symbols = ['aapl', 'apple', 'googl', 'google', 'amzn', 'amazon', 'msft', 'microsoft', 'tesla', 'tsla']\n",
    "    \n",
    "    def clean(text):\n",
    "        try:\n",
    "            return {\n",
    "                'NEG': count_negation(text, negating_w),\n",
    "                'count_neg': count_neg(text, neg),\n",
    "                'count_pos': count_pos(text, pos),\n",
    "                'text': tokenize(remove_hashtags(remove_urls(text, pattern)), pos_list, stop_words, symbols)\n",
    "            }\n",
    "        except:\n",
    "            print('error processing: ' + text)\n",
    "            return {\n",
    "                'NEG': 0,\n",
    "                'count_neg': 0,\n",
    "                'count_pos': 0,\n",
    "                'text': ''\n",
    "            }\n",
    "    \n",
    "    # progress bar\n",
    "    tqdm.pandas()\n",
    "    \n",
    "    clean_df = df.progress_apply(lambda row: clean(row.text), axis='columns', result_type='expand')\n",
    "    return pd.concat([df.drop(columns='text'), clean_df], axis='columns')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF Vectorizer\n",
    "def create_feature_set(df_train, df_test, max_feat = 10000, test_size=0.1, random_state=6242):\n",
    "    vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features = max_feat, min_df = 3)\n",
    "    # Split into training and test set\n",
    "#     df_train, df_test = train_test_split(df, test_size = test_size, random_state = random_state)\n",
    "\n",
    "    # Fit vectoriser on train set\n",
    "    vectoriser.fit(df_train.text)\n",
    "\n",
    "    # Transform data using vectoriser\n",
    "    x_train_tfidf = vectoriser.transform(df_train.text)\n",
    "    x_test_tfidf = vectoriser.transform(df_test.text)\n",
    "    \n",
    "    # If the dataset contains the lexicon based features\n",
    "    if {'NEG', 'count_neg', 'count_pos'}.issubset(set(df_train.columns)):\n",
    "        \n",
    "        # Create Sparse Matrix from lexicon features\n",
    "        x_train_lex = sp.sparse.csr_matrix(df_train[['NEG', 'count_neg', 'count_pos']])\n",
    "        x_test_lex = sp.sparse.csr_matrix(df_test[['NEG', 'count_neg', 'count_pos']])\n",
    "\n",
    "        # Combine Tf-Idf and lexicon features\n",
    "        x_train = sp.sparse.hstack([x_train_tfidf, x_train_lex])\n",
    "        x_test = sp.sparse.hstack([x_test_tfidf, x_test_lex])\n",
    "        \n",
    "    else:\n",
    "        x_train = x_train_tfidf\n",
    "        x_test = x_test_tfidf\n",
    "\n",
    "    y_train = df_train.target.astype(int)\n",
    "#     y_test = df_test.target.astype(int)\n",
    "    \n",
    "    return x_train, x_test, y_train, vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_cfm(y_test, y_pred, ax, title):\n",
    "#     cfm = confusion_matrix(y_test, y_pred)\n",
    "#     categories  = ['Negative','Positive']\n",
    "#     group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "#     group_percentages = ['{0:.2%}'.format(value) for value in cfm.flatten() / np.sum(cfm)]\n",
    "\n",
    "#     labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "#     labels = np.asarray(labels).reshape(2,2)\n",
    "    \n",
    "# #     fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,5), dpi=160)\n",
    "#     acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "#     sns.heatmap(cfm, annot = labels, cmap = 'Blues', fmt = '', ax = ax,\n",
    "#                 xticklabels = categories, yticklabels = categories, cbar = False)\n",
    "#     ax.set_xlabel(\"Predicted values\", labelpad = 10)\n",
    "#     ax.set_ylabel(\"Actual values\"   , labelpad = 10)\n",
    "#     ax.set_title (f\"Confusion Matrix - {title}\\naccuracy: {acc:.2%}\", pad = 20, fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(clf, xtrain, ytrain, xtest):\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    ypred = clf.predict(xtest)\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(sample = False, sample_size = 1000, save = False):\n",
    "    # Read in data\n",
    "    print(\"Reading in data\")\n",
    "    train_df = read_sentiment140(\"data/sentiment140.csv\")\n",
    "    test_df = read_stocks(\"data/final_tweets.csv\")\n",
    "\n",
    "    if sample: \n",
    "        # Get sample set\n",
    "        train_df = train_df.sample(n = sample_size)\n",
    "        test_df = test_df.sample(n = sample_size)\n",
    "        \n",
    "    print(\"Performing data cleaning\")\n",
    "    # Performing data cleaning\n",
    "    clean_train = clean_df(train_df)\n",
    "    clean_test = clean_df(test_df)\n",
    "    \n",
    "    print(\"TF-IDF Vectorizer Training\")\n",
    "    # vectorization and train-test split\n",
    "    x_train, x_test, y_train, v = create_feature_set(clean_train, clean_test)\n",
    "    bayes = BernoulliNB(alpha = 0.1)\n",
    "    \n",
    "    print(\"Model Fit and Prediction\")\n",
    "    y_pred = train_model(bayes, x_train, y_train, x_test)\n",
    "    test_df['sentiment'] = y_pred\n",
    "    if save:\n",
    "        test_df.to_csv(\"test_sentiment.csv\")\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    get_sentiment(sample = False, save = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
